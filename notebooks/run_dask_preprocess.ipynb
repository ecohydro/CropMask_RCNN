{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this notebook must be run from the cropmask environment's jupyterlab, otherwise there are kernel restart errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "import xml.etree.ElementTree as et \n",
    "%matplotlib inline\n",
    "\n",
    "root_dir_sr = Path(\"/mnt/cropmaskperm/unpacked_ard_landsat_downloads/ARDSR/\")\n",
    "root_dir_xml = Path(\"/mnt/cropmaskperm/unpacked_ard_landsat_downloads/ARDxml/\")\n",
    "\n",
    "scene_paths = sorted(root_dir_sr.glob(\"*\"))[0]\n",
    "xml_paths = sorted(root_dir_xml.glob(\"*\"))\n",
    "df_cols = [\"cloud_cover\", \"cloud_shadow\", \"snow_ice\", \"fill\", \"instrument\", \"level1_collection\", \"ard_version\"]\n",
    "rows = []\n",
    "\n",
    "for xml_path in xml_paths:\n",
    "    \n",
    "    xtree = et.parse(xml_path)\n",
    "    tile_meta_global = list(xtree.getroot())[0][0]\n",
    "    dataframe_dict = {}\n",
    "\n",
    "    element = tile_meta_global.find(\"{https://landsat.usgs.gov/ard/v1}\"+\"tile_grid\")\n",
    "    h = element.attrib['h']\n",
    "    v = element.attrib['v']\n",
    "    \n",
    "    element = tile_meta_global.find(\"{https://landsat.usgs.gov/ard/v1}\"+\"acquisition_date\")\n",
    "    datetime = pd.to_datetime(element.text, format=\"%Y-%m-%d\")\n",
    "    \n",
    "    dataframe_dict.update({'h':h, 'v':v, 'acquisition_date':datetime})\n",
    "    \n",
    "    for col in df_cols:\n",
    "        element = tile_meta_global.find(\"{https://landsat.usgs.gov/ard/v1}\"+col)\n",
    "        if col in [\"cloud_cover\", \"cloud_shadow\", \"snow_ice\", \"fill\"]:\n",
    "            element.text = float(element.text)\n",
    "        dataframe_dict.update({col:element.text})\n",
    "    rows.append(dataframe_dict)\n",
    "    \n",
    "out_df = pd.DataFrame(rows, columns = df_cols.extend(['acquisition_date', 'h','v']))\n",
    "\n",
    "out_df = out_df.set_index(\"acquisition_date\")\n",
    "\n",
    "out_df['xml_paths'] = xml_paths\n",
    "out_df['scene_paths'] = scene_paths\n",
    "\n",
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drops the cloudiest duplicate by keeping the first duplicate. since we sorted by least cloudy to cloudiest\n",
    "least_cloudy_jul_aug_df = out_df['2005-07-01':\"2005-08-31\"]\\\n",
    "    .sort_values(\"cloud_cover\")\\\n",
    "    .drop_duplicates(['h','v']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_qa = sorted(least_cloudy_jul_aug_df.iloc[-1]['scene_paths'].glob(\"*\"))[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys; print(sys.version)\n",
    "import platform; print(platform.platform())\n",
    "import skimage; print(\"scikit-image version: {}\".format(skimage.__version__))\n",
    "import numpy; print(\"numpy version: {}\".format(numpy.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import skimage.io as skio\n",
    "\n",
    "with rasterio.open(cloud_qa.as_posix()) as src:\n",
    "    arr = src.read()\n",
    "\n",
    "arr\n",
    "\n",
    "skio.imshow(cloud_qa.as_posix())\n",
    "\n",
    "skio.imread(cloud_qa.as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as skio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cropmask.preprocess import PreprocessWorkflow, setup_dirs\n",
    "import time\n",
    "import dask\n",
    "\n",
    "param_path = \"/home/ryan/work/CropMask_RCNN/cropmask/test_preprocess_config.yaml\"\n",
    "\n",
    "# selected scenes with almost no clouds that occurred as well outside of the frost season as possible (ends in February-March)\n",
    "scene_list = [\n",
    "    \"/mnt/cropmaskperm/unpacked_landsat_downloads/LT050320312005082801T1-SC20190418222350\", \n",
    "    \"/mnt/cropmaskperm/unpacked_landsat_downloads/LT050320312005040601T1-SC20190418222326\",\n",
    "    \"/mnt/cropmaskperm/unpacked_landsat_downloads/LT050290312005031601T1-SC20190818204935\",  \n",
    "    \"/mnt/cropmaskperm/unpacked_landsat_downloads/LT050300312005020301T1-SC20190818205734\",\n",
    "    \"/mnt/cropmaskperm/unpacked_landsat_downloads/LT050300322005020301T1-SC20190818205733\",\n",
    "    \"/mnt/cropmaskperm/unpacked_landsat_downloads/LT050290322005031601T1-SC20190818205024\"\n",
    "]\n",
    "labels_path = \"/mnt/cropmaskperm/external/nebraska_pivots_projected.geojson\"\n",
    "\n",
    "setup_dirs(param_path)\n",
    "\n",
    "\n",
    "# this is just to get the train dir path\n",
    "wflow = PreprocessWorkflow(param_path, \n",
    "                             scene_list[0],\n",
    "                             labels_path)\n",
    "\n",
    "results = []\n",
    "for scene_path in scene_list:\n",
    "\n",
    "    wflow = dask.delayed(PreprocessWorkflow)(param_path, scene_path, labels_path)\n",
    "\n",
    "#     wflow = PreprocessWorkflow(param_path, scene_path, labels_path)\n",
    "    \n",
    "    band_list = wflow.yaml_to_band_index()\n",
    "        \n",
    "    product_list = wflow.get_product_paths(band_list)\n",
    "        \n",
    "    a = wflow.load_meta_and_bounds(product_list)\n",
    "        \n",
    "    b = a.stack_and_save_bands()\n",
    "        \n",
    "    c = b.negative_buffer_and_small_filter(-31, 100)\n",
    "        \n",
    "    d = c.grid_images()\n",
    "    \n",
    "    e = d.imgs_to_pngs()\n",
    "        \n",
    "    f = e.connected_components()\n",
    "    \n",
    "    result = f.labels_to_pngs()\n",
    "\n",
    "    results.append(result)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.dask.org/en/stable/delayed-best-practices.html\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()  # use dask.distributed by default\n",
    "\n",
    "x = dask.compute(*results, scheduler=\"single-threaded\")  # start computation in the background\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just to get the train dir path\n",
    "wflow = PreprocessWorkflow(param_path, \n",
    "                             scene_list[0],\n",
    "                             labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wflow\n",
    "band_list = wflow.yaml_to_band_index()\n",
    "        \n",
    "product_list = wflow.get_product_paths(band_list)\n",
    "        \n",
    "a = wflow.load_meta_and_bounds(product_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    means = []\n",
    "    for i in wflow.band_list:\n",
    "        mean = get_arr_channel_mean(wflow.TRAIN,int(i)-1)\n",
    "        means.append(mean)\n",
    "        print(\"Band index {} mean for COCO normalization: \".format(i), mean)\n",
    "        \n",
    "    stop = time.time()\n",
    "        \n",
    "    print(stop-start, \" seconds for this number of scenes: \" + str(len(scene_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as skio\n",
    "\n",
    "arr = skio.imread(\"/mnt/cropmaskperm/test-landsat/chips/LT050320312005040601T1-SC20190418222326_tile_1024_3072/mask/LT050320312005040601T1-SC20190418222326_tile_1024_3072_label.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "smaller_arr = arr.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%d megabytes\" % (arr.size * arr.itemsize / (1e6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%d megabytes\" % (smaller_arr.size * smaller_arr.itemsize / (1e6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
