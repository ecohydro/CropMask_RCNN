{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "import xml.etree.ElementTree as et \n",
    "%matplotlib inline\n",
    "\n",
    "root_dir_sr = Path(\"/mnt/cropmaskperm/unpacked_ard_landsat_downloads/ARDSR/\")\n",
    "root_dir_xml = Path(\"/mnt/cropmaskperm/unpacked_ard_landsat_downloads/ARDxml/\")\n",
    "\n",
    "scene_paths = sorted(root_dir_sr.glob(\"*\"))\n",
    "xml_paths = sorted(root_dir_xml.glob(\"*\"))\n",
    "df_cols = [\"cloud_cover\", \"cloud_shadow\", \"snow_ice\", \"fill\", \"instrument\", \"level1_collection\", \"ard_version\"]\n",
    "rows = []\n",
    "\n",
    "\n",
    "for xml_path in xml_paths:\n",
    "    \n",
    "    xtree = et.parse(xml_path)\n",
    "    tile_meta_ard = list(xtree.getroot())[0][0]\n",
    "    tile_meta_global = list(xtree.getroot())[1][1]\n",
    "    dataframe_dict = {}\n",
    "\n",
    "    element = tile_meta_ard.find(\"{https://landsat.usgs.gov/ard/v1}\"+\"tile_grid\")\n",
    "    h = element.attrib['h']\n",
    "    v = element.attrib['v']\n",
    "    \n",
    "    element = tile_meta_global.find(\"{https://landsat.usgs.gov/ard/v1}\"+\"wrs\")\n",
    "    path = element.attrib['path']\n",
    "    row = element.attrib['row']\n",
    "    \n",
    "    element = tile_meta_ard.find(\"{https://landsat.usgs.gov/ard/v1}\"+\"acquisition_date\")\n",
    "    datetime = pd.to_datetime(element.text, format=\"%Y-%m-%d\")\n",
    "    \n",
    "    dataframe_dict.update({'h':h, 'v':v, 'path': int(path), 'row': int(row), 'acquisition_date':datetime})\n",
    "    \n",
    "    for col in df_cols:\n",
    "        element = tile_meta_ard.find(\"{https://landsat.usgs.gov/ard/v1}\"+col)\n",
    "        if col in [\"cloud_cover\", \"cloud_shadow\", \"snow_ice\", \"fill\"]:\n",
    "            element.text = float(element.text)\n",
    "        dataframe_dict.update({col:element.text})\n",
    "    rows.append(dataframe_dict)\n",
    "    \n",
    "out_df = pd.DataFrame(rows, columns = df_cols.extend(['h','v', 'path', 'row', 'acquisition_date']))\n",
    "\n",
    "out_df = out_df.set_index(\"acquisition_date\")\n",
    "\n",
    "out_df['xml_paths'] = xml_paths\n",
    "out_df['scene_paths'] = scene_paths\n",
    "\n",
    "# original paths and rows used in (path, row) form\n",
    "og_path_rows = [(29, 31), (29, 32), (30, 31), (30, 32), (31, 31), (31, 32), (32, 31), (32, 32), (33, 31), (33, 32)]\n",
    "# below not used yet, unsure which dates correspond to which path rows since this isn't documented in the label metadata\n",
    "og_dates = pd.to_datetime([\"2005/06/20\", '2005/07/22', '2005/09/08', '2005/06/27', '2005/08/30', '2005/09/15', '2005/08/05', '2005/09/06', '2005/07/11', '2005/08/28', '2005/07/02', '2005/08/19', '2005/09/20'])\n",
    "# 2005-07-20 was missing, could be a mistake in the metadata, amybe they meant 2005-07-22\n",
    "\n",
    "out_df = out_df.loc[og_dates]\n",
    "\n",
    "out_df = out_df[out_df[['path', 'row']].apply(tuple, axis=1).isin(og_path_rows)]\n",
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drops the cloudiest duplicate by keeping the first duplicate. since we sorted by least cloudy to cloudiest\n",
    "least_cloudy_june_sept_df = out_df['2005-06-20':\"2005-09-20\"]\\\n",
    "    .sort_values([\"fill\", \"cloud_cover\"])\\\n",
    "    .drop_duplicates(['h','v']) \n",
    "\n",
    "least_cloudy_june_sept_lst = least_cloudy_june_sept_df['scene_paths'].apply(str).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cropmask.preprocess import PreprocessWorkflow, setup_dirs\n",
    "import time\n",
    "import dask\n",
    "\n",
    "param_path = \"/home/ryan/work/CropMask_RCNN/cropmask/test_preprocess_config.yaml\"\n",
    "\n",
    "# selected scenes with almost no clouds that occurred as well outside of the frost season as possible (ends in February-March)\n",
    "scene_list = least_cloudy_june_sept_lst\n",
    "labels_path = \"/mnt/cropmaskperm/external/nebraska_pivots_projected.geojson\"\n",
    "\n",
    "setup_dirs(param_path)\n",
    "\n",
    "results = []\n",
    "for scene_path in scene_list:\n",
    "\n",
    "#     wflow = dask.delayed(PreprocessWorkflow)(param_path, scene_path, labels_path)\n",
    "\n",
    "    wflow = PreprocessWorkflow(param_path, scene_path, labels_path)\n",
    "    \n",
    "    band_list = wflow.yaml_to_band_index()\n",
    "        \n",
    "    product_list = wflow.get_product_paths(band_list)\n",
    "        \n",
    "    a = wflow.load_meta_and_bounds(product_list)\n",
    "        \n",
    "    b = a.stack_and_save_bands()\n",
    "        \n",
    "    c = b.tile_scene_and_vector()\n",
    "    \n",
    "    result = c.geojsons_to_masks() #not needed for coco conversions tep if using solaris\n",
    "\n",
    "    results.append(result)\n",
    "  \n",
    "\n",
    "# # # https://docs.dask.org/en/stable/delayed-best-practices.html\n",
    "# from dask.distributed import Client\n",
    "\n",
    "# client = Client()  # use dask.distributed by default\n",
    "\n",
    "# x = client.compute(results, scheduler=\"processes\", num_workers=3)  # start computation in the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from cropmask.preprocess import PreprocessWorkflow, setup_dirs\n",
    "import time\n",
    "import dask\n",
    "\n",
    "param_path = \"/home/ryan/work/CropMask_RCNN/cropmask/test_preprocess_config.yaml\"\n",
    "\n",
    "# selected scenes with almost no clouds that occurred as well outside of the frost season as possible (ends in February-March)\n",
    "# scene_list = least_cloudy_june_sept_lst\n",
    "labels_path = \"/mnt/cropmaskperm/external/nebraska_pivots_projected.geojson\"\n",
    "\n",
    "setup_dirs(param_path)\n",
    "# problem path\n",
    "wflow = PreprocessWorkflow(param_path, \n",
    "                             \"/mnt/cropmaskperm/unpacked_ard_landsat_downloads/ARDSR/LT05_CU_016008_20050620_20190102_C01_V01_SR\",\n",
    "                             labels_path)\n",
    "\n",
    "band_list = wflow.yaml_to_band_index()\n",
    "\n",
    "product_list = wflow.get_product_paths(band_list)\n",
    "\n",
    "a = wflow.load_meta_and_bounds(product_list)\n",
    "\n",
    "b = a.stack_and_save_bands()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "_ = gpd.read_file(\"/mnt/cropmaskperm/external/nebraska_pivots_projected.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_[_.is_valid==False]['geometry'].iloc[0].intersection(_[_.is_valid==False]['geometry'].iloc[0]).is_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_[_.is_valid==False]['geometry'] = _[_.is_valid==False].buffer(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_not_valid.buffer(0).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_not_valid.iloc[0]['geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_not_valid['geometry'].buffer(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_not_valid['geometry'].convex_hull.iloc[4].intersection(all_not_valid.iloc[4]['geometry'].buffer(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.tile_scene_and_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import types\n",
    "# wflow.geojsons_to_masks = types.MethodType(geojsons_to_masks, wflow )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rasterizing 915 512x512 tiles that have anyware from 0 to 100 instances took 18 minutes and 32 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geopandas as gpd\n",
    "# import solaris as sol\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "wflow.geojsons_to_masks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xa\n",
    "import rioxarray\n",
    "label = xa.open_rasterio(sorted(wflow.rasterized_label_paths)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rast = xa.open_rasterio(sorted(wflow.raster_tile_paths)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "rast.where(rast>0).plot.imshow(ax=ax, robust=True)\n",
    "label.any(axis=0).where(label.any(axis=0)>0).plot.imshow(ax=ax, alpha=.5, add_colorbar=False)\n",
    "# for i in np.arange(label.shape[0]):\n",
    "#     label[i].plot.imshow(ax=ax, alpha=.5, add_colorbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code graveyard, shape preserving tiling but non uniform shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import box, Polygon, MultiPolygon, GeometryCollection\n",
    "\n",
    "def katana(geometry, approx_tile_size, use_projection_size=False, transform=None, recursion_max = 5000, count = 0):\n",
    "    \"\"\"Split a Polygon into two parts across it's shortest dimension\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    geometry : str, optional\n",
    "        A shapely.geometry.Polygon, path to a single feature geojson, \n",
    "    or list-like bounding box shaped like [left, bottom, right, top]\n",
    "    src_tile_size : `tuple` of `int`s, optional\n",
    "        The size of the input tiles in ``(y, x)`` coordinates. By default,\n",
    "        this is in pixel units; this can be changed to metric units using the\n",
    "        `use_metric_size` argument.\n",
    "    use_metric_size : bool, optional\n",
    "        Is `src_tile_size` in pixel units (default) or metric? To set to metric\n",
    "        use ``use_metric_size=True``.\n",
    "    transform : `tuple` of `int`s, optional\n",
    "        A rasterio transform.\n",
    "    \n",
    "    Adapted from @lossyrob's Gist https://gist.github.com/lossyrob/7b620e6d2193cb55fbd0bffacf27f7f2\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    if isinstance(geometry, str):\n",
    "        gj = json.loads(open(geometry).read())\n",
    "        features = gj['features']\n",
    "        if not len(features) == 1:\n",
    "            print('Feature collection must only contain one feature')\n",
    "            sys.exit(1)\n",
    "        geometry = shape(features[0]['geometry'])\n",
    "        \n",
    "    elif isinstance(geometry, list) or isinstance(geometry, np.ndarray):\n",
    "        assert len(geometry) == 4\n",
    "        geometry = box(*geometry)\n",
    "    \n",
    "    elif isinstance(geometry, (Polygon, MultiPolygon)) is False:\n",
    "        print(\"geometry must be one of type list, numpy.ndarray or shapely.geometry.Polygon\")\n",
    "        return\n",
    "    \n",
    "    bounds = geometry.bounds\n",
    "    width = bounds[2] - bounds[0]\n",
    "    height = bounds[3] - bounds[1]\n",
    "    if use_projection_size is False:\n",
    "        if transform is None:\n",
    "            print(\"\"\"Error: A transform is needed to convert pixel units to \n",
    "                  projection units if use_projection_size is False\"\"\")\n",
    "            return\n",
    "        approx_tile_size = approx_tile_size * transform[0]\n",
    "    if max(width, height) <= approx_tile_size or count == recursion_max:\n",
    "        # either the max dimesnion of the polygon is smaller than the threshold, \n",
    "        # or the maximum number of recursions has been reached\n",
    "        return [geometry]\n",
    "    if height >= width:\n",
    "        # split left to right\n",
    "        a = box(bounds[0], bounds[1], bounds[2], bounds[1]+height/2)\n",
    "        b = box(bounds[0], bounds[1]+height/2, bounds[2], bounds[3])\n",
    "    else:\n",
    "        # split top to bottom\n",
    "        a = box(bounds[0], bounds[1], bounds[0]+width/2, bounds[3])\n",
    "        b = box(bounds[0]+width/2, bounds[1], bounds[2], bounds[3])\n",
    "    result = []\n",
    "    for d in (a, b,):\n",
    "        c = geometry.intersection(d)\n",
    "        if not isinstance(c, GeometryCollection):\n",
    "            c = [c]\n",
    "        for e in c:\n",
    "            if isinstance(e, (Polygon, MultiPolygon)):\n",
    "                result.extend(katana(e, approx_tile_size, count=count+1, use_projection_size=use_projection_size, transform=transform))\n",
    "    if count > 0:\n",
    "        return result\n",
    "    # convert multipart into singlepart\n",
    "    final_result = []\n",
    "    for g in result:\n",
    "        if isinstance(g, MultiPolygon):\n",
    "            final_result.extend(g)\n",
    "        else:\n",
    "            final_result.append(g)\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = katana(neb, 15360, use_projection_size = True)\n",
    "\n",
    "gpd.GeoDataFrame(geometry=[neb]).plot()\n",
    "\n",
    "\n",
    "gpd.GeoDataFrame(geometry=shapes)[400:401].area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "lst = [1,2,3,4,5,6]\n",
    "output_lst = []\n",
    "for i in lst:\n",
    "    output_lst.append(dask.delayed(sum)([i,1]))\n",
    "results = dask.compute(*output_lst)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "f = gpd.read_file(\"/mnt/cropmaskperm/external/nebraska_pivots_projected.geojson\")\n",
    "f.crs\n",
    "\n",
    "import xarray\n",
    "import rioxarray\n",
    "crs = xarray.open_rasterio(list(Path(scene_path).glob(\"*\"))[6]).rio.crs\n",
    "\n",
    "crs\n",
    "\n",
    "f.crs = crs\n",
    "\n",
    "f = f.to_crs(crs)\n",
    "\n",
    "f.crs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
