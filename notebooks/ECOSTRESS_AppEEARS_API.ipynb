{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h1>Using the AρρEEARS API in an ECOSTRESS ARD Workflow - Getting Started</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Objective\n",
    "The intent of this tutorial is to familiarize Landsat Analysis Ready Data ([ARD](https://www.usgs.gov/land-resources/nli/landsat/us-landsat-analysis-ready-data?qt-science_support_page_related_con=0#qt-science_support_page_related_con)) users with the [AρρEEARS](https://lpdaac.usgs.gov/tools/appeears/) application programming interface (API) with demonstrations on how the API, and the services it provides, can be leveraged in an analysis workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Use Case\n",
    "This tutorial was developed using a real-world use case for a project being completed by the [NASA DEVELOP Node at the Marshall Space Flight Center](https://develop.larc.nasa.gov/nodes/MSFC.html). **NASA Develop is a program aimed at integrating NASA Earth observations with society to foster future innovation and cultivate the professionals of tomorrow by addressing diverse environmental issues today.** \n",
    "\n",
    "The example use case comes from a project titled, \"Utilizing NASA Earth Observations to Assess Coastline Replenishment Initiatives and Shoreline Risk along Delaware's Coasts\". The group is partnering with the Delaware Department of Natural Resources and Environmental Control, Division of Watershed Stewardship for this project. The goals for the project include to identify areas of current and potential shoreline loss along the coast of Delaware, assess the current restoration efforts, and **create time-series coastline maps**. \n",
    "\n",
    "### Example: Submit an AppEEARS area request for a portion of the Delaware coast along the Prime Hook National Wildlife Refuge to extract Landsat Analysis Ready Data for the years before and after Hurricane Sandy. The outputs will be used to generate false color composite time series to visualize changes to the coastline during the time period. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Topics Covered\n",
    "1. [**Getting Started**](#gettingstarted)  \n",
    "    1.1 [Enable Access to the API](#1.1)  \n",
    "    1.2 [Login](#1.2)  \n",
    "2. [**Submit an Area Request**](#submittask)  \n",
    "    2.1 [Import a Shapefile](#2.1)  \n",
    "    2.2 [Compile the JSON payload to submit to AρρEEARS](#2.2)  \n",
    "    2.3 [Submit a task request](#2.3)  \n",
    "    2.4 [Get task status](#2.4)  \n",
    "3. [**Download a Request [Bundle API]**](#downloadrequest)  \n",
    "    3.1 [List files associated with the request](#3.1)  \n",
    "    3.2 [Download files in a request](#3.2)  \n",
    "4. [**Explore AρρEEARS Outputs**](#explore)  \n",
    "    4.1 [Open and explore data using xarray](#4.1)  \n",
    "    4.2 [Visualize Time Series Data](#4.2)  \n",
    "5. [**Quality Filtering**](#qualityfiltering)  \n",
    "    5.1 [Decode quality values](#5.1)  \n",
    "    5.2 [Create and apply quality mask](#5.2)  \n",
    "    5.3 [Plot quality filtered data](#5.3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AρρEEARS Information\n",
    "To access AρρEEARS, visit: https://lpdaacsvc.cr.usgs.gov/appeears/\n",
    "\n",
    "For comprehensive documentation of the full functionality of the [AρρEEARS API](https://lpdaacsvc.cr.usgs.gov/appeears/api/), please see the AρρEEARS API Documentation: https://lpdaacsvc.cr.usgs.gov/appeears/api/\n",
    "\n",
    "Throughout the exercise, specific sections of the API documentation can be accessed by clicking the hyperlinked text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies \n",
    "It is recommended to use [Conda](https://conda.io/docs/), an environment manager to set up a compatible Python environment. Download Conda for your OS here: https://www.anaconda.com/download/. Once you have Conda installed, Follow the instructions below to successfully setup a Python environment on MacOS or Windows.\n",
    "\n",
    "This Python Jupyter Notebook tutorial has been tested using Python versions 3.6, 3.6.6 and 3.7.\n",
    "\n",
    "Conda was used to create the python environment.  \n",
    "- **Option 1**: Download the [environment yml file](https://git.earthdata.nasa.gov/projects/LPDUR/repos/landsat-ard-appeears-api/browse/environment.yml):  \n",
    "    - Open the environment.yml file and change the prefix (last line) to the directory on your OS where you want to create the environment (ex: C:\\Username\\Anaconda3\\envs\\ardtutorial) and save the environment file.  \n",
    "    - Using Command Prompt, Anaconda Prompt, Cmder, Terminal, or your preferred command line interface, navigate to the directory where you saved the `environment.yml` file.  \n",
    "    - Type `conda env create -f environment.yml`  \n",
    "    - Type `activate ardtutorial`\n",
    "\n",
    "\n",
    "- **Option 2**: Download each package separately\n",
    "    - Windows OS  or macOS\n",
    "    `conda create -n ardtutorial python=3.6`  \n",
    "- If you already had conda installed on your OS, it is recommended that you update to the latest version:  \n",
    "    `conda update -n base -c defaults conda`\n",
    "- Required Python packages were installed from the conda-forge channel. Installing packages from the conda-forge channel is done by adding conda-forge to your channels with:  \n",
    "`conda config --add channels conda-forge`  \n",
    "- Activate the newly created environment using the command: `activate ardtutorial`  \n",
    "- Required packages needed for this exercise are listed below. \n",
    "    - requests  \n",
    "    `conda install requests`  \n",
    "    - pandas  \n",
    "    `conda install pandas`  \n",
    "    - geopandas  \n",
    "    `conda install geopandas`  \n",
    "    - xarray  \n",
    "    `conda install xarray`  \n",
    "    - numpy  \n",
    "    `conda install numpy`  \n",
    "    - netcdf4  \n",
    "    `conda install netcdf4`  \n",
    "    - holoviews  \n",
    "    `conda install holoviews`  \n",
    "    - pyviz &emsp;&emsp;**NOTE** - [PyViz](http://pyviz.org/) is installed using the pyviz channel not conda-forge.  \n",
    "    `conda install -c pyviz hvplot`  \n",
    "    > If you encounter an issue downloading hvplot using conda, try `pip install pyviz hvplot`  \n",
    "    \n",
    "### Next, download the [Jupyter Notebook](https://git.earthdata.nasa.gov/projects/LPDUR/repos/landsat-ard-appeears-api/browse/ARD_AppEEARS_API.ipynb) and [example shapefile](https://git.earthdata.nasa.gov/projects/LPDUR/repos/landsat-ard-appeears-api/browse/PrimeHookNWR_6kmBuffer.shp) to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Getting Started <a id=\"gettingstarted\"></a>\n",
    "If this is your first time using the [AρρEEARS API](https://lpdaacsvc.cr.usgs.gov/appeears/api/), you must first enable API access by following the instructions provided below after signing in with your [NASA Earthdata Login](https://urs.earthdata.nasa.gov/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1.1 Enable Access to the API <a id=\"1.1\"></a>\n",
    "> To enable access to the [AρρEEARS API](https://lpdaacsvc.cr.usgs.gov/appeears/api/), navigate to the [AρρEEARS website](https://lpdaacsvc.cr.usgs.gov/appeears/). Click the *Sign In* button in the top right portion of the AρρEEARS landing page screen.  \n",
    "\n",
    "<table><tr><td>\n",
    "    <img src=\"https://lpdaacsvc.cr.usgs.gov/assets/images/help/image001.7f0d8820.png\" />\n",
    "</td></tr></table>  \n",
    "\n",
    "> Once you are signed in, click the *Manage User* icon in the top right portion of the AρρEEARS landing page screen and select *Settings*.   \n",
    "\n",
    "<table><tr><td>\n",
    "    <img src=\"https://lpdaacsvc.cr.usgs.gov/assets/images/help/api/image001.3bb7c98a.png\" />\n",
    "</td></tr></table>  \n",
    "\n",
    "> Select the *Enable API* box to gain access to the AρρEEARS API.  \n",
    "\n",
    "<table><tr><td>\n",
    "    <img src=\"https://lpdaacsvc.cr.usgs.gov/assets/images/help/api/image002.ebbb9431.png\" />\n",
    "</td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Login to AρρEEARS/Earthdata <a id=\"1.2\"></a>\n",
    "> To submit a request, you must first [login](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=Python%203#login) to the AρρEEARS API using your Earthdata login credentials.  We’ll use the `getpass` package to conceal our Earthdata login username and password. When executed, the code below will prompt you to enter your username followed by your password and store them as variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the required packages and set the input/working directory to run this Jupyter Notebook locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required Python packages\n",
    "import requests\n",
    "import getpass\n",
    "import time\n",
    "import os\n",
    "import cgi\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray\n",
    "import numpy as np\n",
    "import hvplot.xarray\n",
    "import holoviews as hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input directory, change working directory\n",
    "inDir = \"/scratch/rave/\"  # Set input directory to the current working directory\n",
    "os.chdir(inDir)               # Change to working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\" >\n",
    "<b>If you have downloaded the tutorial materials to a different directory than the Jupyter Notebook, `inDir` above needs to be changed.</b>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To submit a request, you must first login to the AρρEEARS API. Use the `getpass` package to enter your NASA Earthdata login **Username** and **Password**. When prompted after executing the code block below, enter your username followed by your password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter Earthdata login credentials\n",
    "username = getpass.getpass('Earthdata Username:')\n",
    "password = getpass.getpass('Earthdata Password:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API = 'https://lpdaacsvc.cr.usgs.gov/appeears/api/'  # Set the AρρEEARS API to a variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the `requests` package to post your username and password. A successful login will provide you with a token to be used later in this tutorial to submit a request. For more information or if you are experiencing difficulties, please see the [API Documentation](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=Python%203#login)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert API URL, call login service, provide credentials & return json\n",
    "login_response = requests.post(f\"{API}/login\", auth=(username, password)).json() \n",
    "del username, password\n",
    "login_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Above, you should see a Bearer Token. The Bearer Token is needed to leverage the AρρEEARS API via HTTP request methods (e.g., POST and GET). Notice that this token will expire approximately 48 hours after being acquired. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the token to a variable\n",
    "token = login_response['token']\n",
    "head = {'Authorization': f\"Bearer {token}\"} \n",
    "head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Submit an Area Request <a id=\"submittask\"></a>\n",
    "The [Tasks](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=Python%203#tasks) service, among other things (see below), is used to submit requests (e.g., POST and GET) to the AρρEEARS system. Each call to the [Tasks](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=Python%203#tasks) service is associated with your user account. Therefore, each of the calls to this service require an authentication token. The [*submit task*](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=Python%203#submit-task) API call provides a way to submit a new request. It accepts data via JSON, query string, or a combination of both. In the example below, we will compile a json and submit a request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No download commented out since it takes a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datapackage import Package\n",
    "\n",
    "# package = Package('https://datahub.io/core/geo-countries/datapackage.json')\n",
    "\n",
    "# countries = package.get_resource(\"countries\")\n",
    "\n",
    "# This takes about 2 minutes, downloads countries geojson in memory.\n",
    "\n",
    "# all_countries_bytes = countries.raw_read()\n",
    "\n",
    "# all_countries_json = json.loads(all_countries_bytes)\n",
    "\n",
    "# Below we can iterate to get the lis tof countries in our data package.\n",
    "\n",
    "# for i in all_countries_json['features']:\n",
    "#     print(i['properties']['ADMIN'])\n",
    "\n",
    "# for i in all_countries_json['features']:\n",
    "#     if i['properties']['ADMIN'] == \"France\":\n",
    "#         france_geo = i\n",
    "\n",
    "# france_geo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download using data-cli, instructions at: https://datahub.io/core/geo-countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{inDir}core/geo-countries/archive/countries.geojson\", \"rb\") as f:\n",
    "    all_countries_geojson = json.loads(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in all_countries_geojson['features']:\n",
    "    if i['properties']['ADMIN'] == \"France\":\n",
    "        france_geo = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "france_geo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Compile the JSON payload to submit to AρρEEARS <a id=\"2.2\"></a>\n",
    "> Many of the required items needed in the AρρEEARS API request payload have multiple options. For example, AρρEEARS has several projections that can be selected for the output. We can use the AρρEEARS API to find out what projections are available. In this example, we are explicitly assigning our projection to the **proj** variable. To find out how to use the AρρEEARS API to list the available options for each parameter, check out the [AρρEEARS API Tutorials](https://git.earthdata.nasa.gov/projects/LPDUR/repos/appeears-api-getting-started/browse) produced by the [LP DAAC](https://lpdaac.usgs.gov/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get('https://lpdaacsvc.cr.usgs.gov/appeears/api/product')\n",
    "product_response = response.json()\n",
    "# create a dictionary indexed by the product name and version\n",
    "products = {p['ProductAndVersion']: p for p in product_response}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products['CU_LC08.001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products['ECO3ANCQA.001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products['ECO3ETPTJPL.001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products['ECO4ESIPTJPL.001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products['ECO4WUE.001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "product_id = 'ECO4WUE.001'\n",
    "\n",
    "def get_layer_from_productid(product_id):\n",
    "    \"\"\"only works for single layer products\"\"\"\n",
    "    response = requests.get('https://lpdaacsvc.cr.usgs.gov/appeears/api/product/{0}'.format(product_id))\n",
    "    layer_response = response.json()\n",
    "    print(layer_response)\n",
    "    return layer_response\n",
    "get_layer_from_productid(product_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_layer_from_productid('ECO4ESIPTJPL.001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_layer_from_productid('ECO3ETPTJPL.001').keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_layer_from_productid('ECO3ANCQA.001').keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'France ET'    # User-defined name of the task\n",
    "task_type = 'area'                                    # Type of task, area or point\n",
    "proj = \"geographic\"                             # Set output projection \n",
    "outFormat = 'geotiff'                                 # Set output file format type\n",
    "startDate = '08-31-2019'                              # Start of the date range for which to extract data: MM-DD-YYYY\n",
    "endDate = '10-30-2019'                                # End of the date range for which to extract data: MM-DD-YYYY\n",
    "recurring = False                                     # Specify True for a recurring date range\n",
    "\n",
    "# Define the products and layers desired\n",
    "prodLayer = [{\n",
    "        \"layer\": \"L3_L4_QA_ECOSTRESS_L2_QC\",\n",
    "        \"product\": \"ECO3ANCQA.001\"\n",
    "      },\n",
    "      {\n",
    "        \"layer\": \"EVAPOTRANSPIRATION_PT_JPL_ETdaily\",\n",
    "        \"product\": \"ECO3ETPTJPL.001\"\n",
    "      },\n",
    "      {\n",
    "        \"layer\": \"EVAPOTRANSPIRATION_PT_JPL_ETinstUncertainty\",\n",
    "        \"product\": \"ECO3ETPTJPL.001\"\n",
    "      },\n",
    "      {\n",
    "        \"layer\": \"Evaporative_Stress_Index_PT_JPL_PET\",\n",
    "        \"product\": \"ECO4ESIPTJPL.001\"  \n",
    "      }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if tasks are too big for appeears then order one product at a time and/or restrict by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prodLayer = [{\n",
    "        \"layer\": \"L3_L4_QA_ECOSTRESS_L2_QC\",\n",
    "        \"product\": \"ECO3ANCQA.001\"\n",
    "      }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prodLayer = [\n",
    "      {\n",
    "        \"layer\": \"EVAPOTRANSPIRATION_PT_JPL_ETdaily\",\n",
    "        \"product\": \"ECO3ETPTJPL.001\"\n",
    "      }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prodLayer = [\n",
    "      {\n",
    "        \"layer\": \"Evaporative_Stress_Index_PT_JPL_PET\",\n",
    "        \"product\": \"ECO4ESIPTJPL.001\"  \n",
    "      }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the JSON to be submitted as an area request.   \n",
    "Notice that `primehookNWR` is inserted from the shapefile transformed to a geojson via the `geopandas` and `json` packages above in section 2.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "france_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "france_gdf = gpd.GeoDataFrame.from_features([france_geo]).explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "france_gdf[france_gdf.area==max(france_gdf.area)].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi = france_gdf[france_gdf.area==max(france_gdf.area)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi_geojson = json.loads(aoi.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = {\n",
    "    'task_type': task_type,\n",
    "    'task_name': task_name,\n",
    "    'params': {\n",
    "         'dates': [\n",
    "         {\n",
    "             'startDate': startDate,\n",
    "             'endDate': endDate\n",
    "         }],\n",
    "         'layers': prodLayer,\n",
    "         'output': {\n",
    "                 'format': {\n",
    "                         'type': outFormat}, \n",
    "                         'projection': proj},\n",
    "         'geo': aoi_geojson,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The **task** object is what we will submit to the AρρEEARS system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Submit a task request <a id=\"2.3\"></a> \n",
    "> We will now submit our **task** object to AρρEEARS using the [*submit task*](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=Python%203#submit-task) API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post json to the API task service, return response as json\n",
    "task_response = requests.post(f\"{API}/task\", json=task, headers=head)\n",
    "task_response.json()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A task ID is generated for each request and is returned in the response. Task IDs are unique for each request and are used to check request status, explore request details, and list files generated for the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = task_response.json()['task_id']\n",
    "task_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Get task status <a id=\"2.4\"></a>\n",
    "> We can use the [Status](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=Python%203#status) service to retrieve information on the status of all task requests that are currently being processed for your account. We will use the [*task status*](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=Python%203#task-status) API call with our **task_id** to get information on the request we just submitted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_response = requests.get(f\"{API}/status/{task_id}\", headers=head)\n",
    "status_response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For longer running requests we can gently ping the API to get the status of our submitted request using the snippet below. Once the request is complete, we can move on to downloading our request contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use while statement to ping the API every 20 seconds until a response of 'done' is returned\n",
    "starttime = time.time()\n",
    "while requests.get(f\"{API}/task/{task_id}\", headers=head).json()['status'] != 'done':\n",
    "    print(requests.get(f\"{API}/task/{task_id}\", headers=head).json()['status'])\n",
    "    time.sleep(20.0 - ((time.time() - starttime) % 20.0))\n",
    "print(requests.get(f\"{API}/task/{task_id}\", headers=head).json()['status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Task ID at the start of a session, in case you don't do steps 1-2 above becaus ethey are already complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_response[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\n",
    "    'https://lpdaacsvc.cr.usgs.gov/appeears/api/task', \n",
    "    headers=head)\n",
    "task_response = response.json()\n",
    "print(task_response)\n",
    "\n",
    "task_id = task_response[3]['task_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Download a Request <a id=\"downloadrequest\"></a>\n",
    "The [Bundle](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=Python%203#bundle) service provides information about completed tasks (i.e., tasks that have a status of **done**). A bundle will be generated containing all of the files that were created as part of the task request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 List files associated with the request  <a id=\"3.1\"></a>\n",
    "> The [list files](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=Python%203#list-files) API call lists all of the files contained in the bundle which are available for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = requests.get(f\"{API}/bundle/{task_id}\").json()    # Call API and return bundle contents for the task_id as json\n",
    "bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle['task_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bundle_size_gb(bundle):\n",
    "    filesizes_gb = [i['file_size']/1e9 for i in bundle['files']]\n",
    "    return np.sum(filesizes_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Download files in a request <a id=\"3.2\"></a>\n",
    ">The [download file](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=Python%203#download-file) API call gives us the information needed to download all, or a subset of the files available for a request. Just as the task has a **task_id** to identify it, each file in the bundle will also have a unique **file_id** which should be used for any operation on that specific file. The `Content-Type` and `Content-Disposition` headers will be returned when accessing each file to give more details about the format of the file and the filename to be used when saving the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The `bundle` variable we created has more information than we need to download the files. We will first create a python dictionary to hold the **file_id** and associated **file_name** for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "for f in bundle['files']: \n",
    "    files[f['file_id']] = f['file_name']    # Fill dictionary with file_id as keys and file_name as values\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now we will download the files using the **file_ids** from the dictionary into an output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up output directory on local machine\n",
    "outDir = f'{inDir}ecostress-rhone/'\n",
    "if not os.path.exists(outDir):\n",
    "    os.makedirs(outDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the `files` dictionary and a `for` loop to automate downloading all of the output files into the output directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    download_response = requests.get(f\"{API}/bundle/{task_id}/{file}\", stream=True)                                   # Get a stream to the bundle file\n",
    "    filename = os.path.basename(cgi.parse_header(download_response.headers['Content-Disposition'])[1]['filename'])    # Parse the name from Content-Disposition header \n",
    "    filepath = os.path.join(outDir, filename)                                                                         # Create output file path\n",
    "    with open(filepath, 'wb') as file:                                                                                # Write file to dest dir\n",
    "        for data in download_response.iter_content(chunk_size=8192): \n",
    "            file.write(data)\n",
    "print(\"Downloading complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Explore AρρEEARS Outputs <a id=\"explore\"></a>\n",
    "Now that we have downloaded all the files from our request, let's start to check out our data! In our AρρEEARS request, we set the output format to 'netcdf4'. As a result, we have only one output data file. We will open the dataset as an `xarray Dataset` and start to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Open and explore data using [`xarray`](http://xarray.pydata.org/en/stable/) <a id=\"4.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [`Xarray`](http://xarray.pydata.org/en/stable/) extends and combines much of the core functionality from both the Pandas library and Numpy, hence making it very good at handling multi-dimensional (N-dimensional) datasets that contain labels (e.g., variable names or dimension names). Let's open the netcdf file with our data as an xarray object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(outDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xarray.open_rasterio(f'{outDir}ECO3ETPTJPL.001_EVAPOTRANSPIRATION_PT_JPL_ETcanopy_doy2018225152941_aid0001.tif')\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds = xarray.open_dataset(f'{outDir}CU_LE07.001_30m_aid0001.nc')  # Open the L7 ARD output NC4 file from AppEEARS\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Xarray has two fundamental  data structures. A `Dataset` holds multiple variables that potentially share the same coordinates and global metadata for the file (see above). A `DataArray` contains a single multi-dimensional variable and its coordinates, attributes, and metadata. Data values can be pulled out of the DataArray as a `numpy.ndarray` using the `values` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ds.SRB3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ds.SRB3.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can also pull out information for each coordinate item (e.g., lat, lon, time). Here we pull out the *time* coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The `cftime.DatetimeJulian` format of the time coordinate is a little problematic for some plotting libraries and analysis routines. We are going to [convert the time coordinate](https://stackoverflow.com/questions/55786995/converting-cftime-datetimejulian-to-datetime) to the more useable datetime format `datetime64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "datatimeindex = ds.indexes['time'].to_datetimeindex(); # Convert to datetime64\n",
    "ds['time'] = datatimeindex                             # Set converted index to dataset time coordinate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Visualize Time Series Data <a id=\"4.2\"></a>\n",
    "#### Below, use the [`hvPlot`](https://hvplot.pyviz.org/index.html) and [`holoviews`](https://www.holoviews.org/) packages to create an interactive time series plot of the Landsat 7 ARD data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv_ds = hv.Dataset(ds) # Convert to holoviews dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the holoviews dataset as a four dimensional RGB false color composite, defining the x, y, and time dims from the coordinates, and also the fourth dimension which defines where to put each data variable in the RGB composite. \n",
    "> Here we are using a Landsat 7 false color composite combination of:  \n",
    "- R = B4 (NIR)  \n",
    "- G = B5 (SWIR 1)\n",
    "- B = B3 (RED)  \n",
    "#### This false color combination was chosen because it highlights land-water boundaries and is useful in analysis of soil conditions. Since we are interested in analyzing the coastal shoreline, this is a good combination in order to maximize our ability to delineate between land and water features and highlight the shoreline. Vegetation will appear in shades of green/orange/brown, with increasingly saturated soils appearing in very dark colors. Water will appear very dark, almost black. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the .to() method to plot the holoviews dataset as an RGB, defining the x/y/z dimensions and data variables used\n",
    "timeSeries = hv_ds.to(hv.RGB, kdims=[\"xdim\", \"ydim\"], dynamic=True, vdims=[\"SRB4\",\"SRB5\",\"SRB3\"])\n",
    "timeSeries.opts(width=600, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Above, visualize the  multidimensional (t,x,y) plot of our gridded data. Move the slide on the right to visualize the different time slices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice in the visualization above that there are many time slices with only fill values. This is easily explained when considering that the Landsat 7 data has been tiled and gridded into an Analysis Ready Data stack. Thus, for the observations with all fill values, there are Landsat 7 data that exist in the ARD tile, however outside of the extent of our region of interest (ROI). Below, remove all observations that only contain fill values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a list to store time slices that contain non-fill value data by going through each time slice for a variable\n",
    "goodTimes = []\n",
    "for i in range(len(ds.time)):\n",
    "    if np.nanmean(ds.SRB4[i,:,:]) > 0:                 # Only keep time slices where mean reflectance is greater than 0\n",
    "        goodTimes.append(ds.SRB4[i,:,:].time.values)   # Append time value for valid observation to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodTimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Use xarray's powerful indexing method to pull out the `time` coordinates in the `goodTimes` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.sel(time=goodTimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the holoviews dataset again without the empty time slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv_ds = hv.Dataset(ds)\n",
    "timeSeries = hv_ds.to(hv.RGB, kdims=[\"xdim\", \"ydim\"], dynamic=True, vdims=[\"SRB4\",\"SRB5\",\"SRB3\"])\n",
    "timeSeries.opts(width=600, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How about cloudy observations? Below is an example of how to filter out cloudy or poor quality pixels from the image time series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Quality Filtering <a id=\"qualityfiltering\"></a>\n",
    "When available, AρρEEARS extracts and returns quality assurance (QA) data for each data file returned regardless of whether the user requests it. This is done to ensure that the user possesses the information needed to determine the usability and usefulness of the data they get from AρρEEARS. The [Quality](https://lpdaacsvc.cr.usgs.gov/appeears/api/#quality) service from the AρρEEARS API can be leveraged to create masks that filter out undesirable data values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice that the xarray Dataset contains a data array/variable called `PIXELQA`, which has the same dimensions/coordinates as the `SRB#` data arrays/variables. We can use the quality array to create a mask of poor-quality data. We'll use the [Quality](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=Python%203#quality) service to decode the quality assurance information. \n",
    "\n",
    "> We'll use the following criteria to mask out poor quality data:\n",
    "- Cloud (Cloud) == No\n",
    "- Cloud Shadow (CS) == No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Decode quality values <a id=\"5.1\"></a>\n",
    "> We do not want to decode the same value multiple times. Let's extract all of the unique data values from the `PixelQA` xarray DataArray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality Filtering\n",
    "quality_values = pd.DataFrame(np.unique(ds.PIXELQA.values), columns=['value']).dropna()\n",
    "quality_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The following function decodes the data values from the `PIXELQA` xarray DataArray using the [Quality](https://lpdaacsvc.cr.usgs.gov/appeears/api/#quality) service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qualityDecode(qualityservice_url, product, qualitylayer, value):\n",
    "    req = requests.get(f\"{qualityservice_url}/{product}/{qualitylayer}/{value}\")\n",
    "    return(req.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now we will create an empty dataframe to store the decoded quality information for the masking criteria we identified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_desc = pd.DataFrame(columns=['value', 'Cloud_bits', 'Cloud_description', 'CS_bits', 'CS_description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The for loop below goes through all of the unique quality data values, decodes them using the quality service, and appends the quality descriptions to our empty dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in quality_values.iterrows():\n",
    "    decode_int = qualityDecode(f'{API}/quality',\n",
    "                               \"CU_LE07.001\",\n",
    "                               'PIXELQA',\n",
    "                               str(int(row['value'])))\n",
    "    quality_info = decode_int\n",
    "    df = pd.DataFrame({'value': int(row['value']),\n",
    "                       'Cloud_bits': quality_info['Cloud']['bits'], \n",
    "                       'Cloud_description': quality_info['Cloud']['description'], \n",
    "                       'CS_bits': quality_info['Cloud Shadow']['bits'],\n",
    "                       'CS_description': quality_info['Cloud Shadow']['description']}, index=[index])\n",
    "\n",
    "    quality_desc = quality_desc.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Create and apply quality mask <a id=\"5.2\"></a>\n",
    "> Now we have a dataframe with all of the quality information we need to create a quality mask. Next, we'll identify the quality categories that we would like to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep observations where cloud AND cloud shadow both = no (meaning, there are no clouds/shadows present)\n",
    "mask_values = quality_desc[((quality_desc['Cloud_description'] == 'No') &\n",
    "                           (quality_desc['CS_description'] == 'No'))]\n",
    "mask_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's apply the mask to our xarray dataset, keeping only the values that we have deemed acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsMasked = ds.where(ds['PIXELQA'].isin(mask_values['value']))\n",
    "dsMasked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter out any additional observations that may be returning only fill values after applying the cloud mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodTimes = []\n",
    "for i in range(len(dsMasked.time)):\n",
    "    if np.nanmean(dsMasked.SRB4[i,:,:]) > 0:\n",
    "        goodTimes.append(dsMasked.SRB4[i,:,:].time.values)\n",
    "dsFinal = dsMasked.sel(time=goodTimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Plot quality filtered data <a id=\"5.3\"></a>\n",
    "> Using the same plotting functionality from above, let's see how our data looks when we mask out the undesirable pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv_ds = hv.Dataset(dsFinal)\n",
    "timeSeries = hv_ds.to(hv.RGB, kdims=[\"xdim\", \"ydim\"], dynamic=True, vdims=[\"SRB4\",\"SRB5\",\"SRB3\"])\n",
    "timeSeries.opts(width=600, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This tutorial provides a template to use for your own research workflows. Leveraging the AρρEEARS API for extracting and formatting analysis ready data and importing it directly into Python means that you can keep your entire research workflow in a single software program, from start to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h1> Contact Information </h1>\n",
    "    <h3> Material written by Cole Krehbiel$^{1}$ & Aaron Friesz$^{2}$ </h3>\n",
    "    <ul>\n",
    "        <b>Contact:</b> LPDAAC@usgs.gov <br> \n",
    "        <b>Voice:</b> +1-605-594-6116 <br>\n",
    "        <b>Organization:</b> Land Processes Distributed Active Archive Center (LP DAAC) <br>\n",
    "        <b>Website:</b> https://lpdaac.usgs.gov/ <br>\n",
    "        <b>Date last modified:</b> 10-16-2019 <br>\n",
    "    </ul>\n",
    "\n",
    "$^{1}$Innovate! Inc., contractor to the U.S. Geological Survey, Earth Resources Observation and Science (EROS) Center, Sioux Falls, South Dakota, 57198-001, USA. Work performed under USGS contract G15PD00467 for LP DAAC$^{3}$.  \n",
    "\n",
    "$^{2}$KBR Inc., contractor to the U.S. Geological Survey, Earth Resources Observation and Science (EROS) Center, Sioux Falls, South Dakota, 57198-001, USA. Work performed under USGS contract G15PD00467 for LP DAAC$^{3}$.\n",
    "\n",
    "$^{3}$LP DAAC Work performed under NASA contract NNG14HH33I.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:METRIC]",
   "language": "python",
   "name": "conda-env-METRIC-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
